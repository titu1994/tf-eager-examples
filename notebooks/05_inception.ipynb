{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "image_size = 28\n",
    "batch_size = 512\n",
    "epochs = 6\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28, 1)\n",
      "y train (60000, 10)\n",
      "x test (10000, 28, 28, 1)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, image_size, image_size, 1))\n",
    "x_test = x_test.reshape((-1, image_size, image_size, 1))\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a basic Conv-BN-Relu Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNRelu(tf.keras.Model):\n",
    "    def __init__(self, channels, strides=1, kernel=3, padding='same'):\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.conv =  tf.keras.layers.Conv2D(channels, (kernel, kernel), strides=(strides, strides), padding=padding,\n",
    "                                            use_bias=False, kernel_initializer='he_normal')\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an \"Inception\" Module\n",
    "\n",
    "Lets chain together a few branches of Conv-BN-Relu blocks together into a hypercolumn and use that as a \"Inception block\". \n",
    "\n",
    "This is an adhoc model only for detailing that you can have any number of **Model as Layers** depth. We will be using these `InceptionBlocks` as Layers inside the `InceptionCIFAR` model, and these blocks themselves hold `ConvBNRelu` blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(tf.keras.Model):\n",
    "    def __init__(self, channels, strides=1):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "\n",
    "        self.conv1 = ConvBNRelu(channels, strides, kernel=1)\n",
    "        self.conv2 = ConvBNRelu(channels, strides, kernel=3)\n",
    "        self.conv3_1 = ConvBNRelu(channels, strides, kernel=3)\n",
    "        self.conv3_2 = ConvBNRelu(channels, 1, kernel=3)\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')\n",
    "        self.maxpool_conv = ConvBNRelu(channels, strides, kernel=1)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x1 = self.conv1(inputs, training=training)\n",
    "\n",
    "        x2 = self.conv2(inputs, training=training)\n",
    "\n",
    "        x3_1 = self.conv3_1(inputs, training=training)\n",
    "        x3_2 = self.conv3_2(x3_1, training=training)\n",
    "\n",
    "        x4 = self.maxpool(inputs)\n",
    "        x4 = self.maxpool_conv(x4, training=training)\n",
    "\n",
    "        x = tf.keras.layers.concatenate([x1, x2, x3_2, x4], axis=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Configurable Network\n",
    "\n",
    "This network is adaptive, in that it can have many layers, and therefore we cannot determine the layers before hand.\n",
    "\n",
    "To remedy this, we use the convenient `setattr` (and optinally `getattr`) to dynamically \"register\" and \"call\" sublayers.\n",
    "\n",
    "# Note on why this is needed\n",
    "\n",
    "Eager Models *will* automatically register all variables that have been bound to an identifier inside that class - \n",
    "\n",
    "- Using `self.layer_name = tf.keras.layers.***`\n",
    "- Using `self.block = ClassWhichInheritsModel(...)`\n",
    "\n",
    "However. **it will not register variables that have not been bound directly to the class itself or are custom variables.**\n",
    "\n",
    "- Using `self.layers = [layer1, layer2]`\n",
    "- Using `self.layers = {'l1':layer1, 'l2':layer2}`\n",
    "- Using `self.variable = tf.get_variable(...)`\n",
    "\n",
    "Special case : \n",
    "\n",
    "- Using `self.cells = [LSTMCell(), LSTMCell()]` and then wrapping it around an RNN as : `self.rnn = RNN(self.cells)` **will work as expected**. The weights of the LSTMCell will be registered and the RNN itself is registered as well.\n",
    "\n",
    "**`setattr` and `getattr` bypasses the above issues, and sets the layers or models to the class itself, so it is registered by Keras.**\n",
    "\n",
    "# Note 2\n",
    "\n",
    "This registration of layers is important only for convenience of using Model methods - when using Model.compile(), Model.fit(), Model.predict() and Model.evaluate().\n",
    "\n",
    "If there is no need for these utilities, you can write the class as you want, extract all the variables in a list, get the gradients using `tf.GradientTape()` and then update the parameters by hand using `Optimizer.apply_gradients()`. In such a scenario, even the **Model._set_input(...)** fix need not be applied, since you will be doing batch level training anyways and the first update will use that small batch to determine the shape of the model. Such an example is shown in `10_custom_model.ipynb`\n",
    "\n",
    "However, it is far too convenient to use Keras' inbuilt methods for general use-cases such as classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionCIFAR(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_classes, initial_filters=16, **kwargs):\n",
    "        super(InceptionCIFAR, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = initial_filters\n",
    "        self.out_channels = initial_filters\n",
    "        self.num_layers = num_layers\n",
    "        self.initial_filters = initial_filters\n",
    "\n",
    "        self.conv1 = ConvBNRelu(initial_filters)\n",
    "\n",
    "        self.blocks = []\n",
    "\n",
    "        # build all the blocks\n",
    "        for block_id in range(num_layers):\n",
    "            for layer_id in range(2):  # 2 layers per block \n",
    "                key = 'block_%d_%d' % (block_id + 1, layer_id + 1)\n",
    "                if layer_id == 0:\n",
    "                    block = InceptionBlock(self.out_channels, strides=2)\n",
    "                else:\n",
    "                    block = InceptionBlock(self.out_channels)\n",
    "\n",
    "                self.in_channels = self.out_channels\n",
    "                \n",
    "                # \"register\" this block to this model. Without this, weights wont update.\n",
    "                setattr(self, key, block)\n",
    "                \n",
    "                self.blocks.append(block)\n",
    "\n",
    "            self.out_channels *= 2\n",
    "\n",
    "        self.avg_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.fc = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        out = self.conv1(inputs, training=training)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            out = block(out, training=training)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables in the model : 107\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_bn_relu_1 (ConvBNRelu)  multiple                  208       \n",
      "_________________________________________________________________\n",
      "inception_block_1 (Inception multiple                  7744      \n",
      "_________________________________________________________________\n",
      "inception_block_2 (Inception multiple                  23104     \n",
      "_________________________________________________________________\n",
      "inception_block_3 (Inception multiple                  50816     \n",
      "_________________________________________________________________\n",
      "inception_block_4 (Inception multiple                  91776     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 174,938\n",
      "Trainable params: 173,946\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 21s 344us/step - loss: 0.1963 - acc: 0.9459 - val_loss: 0.6752 - val_acc: 0.8465\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 0.0339 - acc: 0.9898 - val_loss: 0.2682 - val_acc: 0.9254\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.0263 - acc: 0.9921 - val_loss: 0.0874 - val_acc: 0.9739\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 20s 332us/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0766 - val_acc: 0.9758\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 0.0155 - acc: 0.9951 - val_loss: 0.1425 - val_acc: 0.9554\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 20s 331us/step - loss: 0.0161 - acc: 0.9945 - val_loss: 0.0503 - val_acc: 0.9864\n",
      "10000/10000 [==============================] - 1s 95us/step\n",
      "Final test loss and accuracy : [0.050301548755168915, 0.9863999997138977]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = InceptionCIFAR(2, num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = np.zeros((1, image_size, image_size, 1))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    print(\"Number of variables in the model :\", len(model.variables))\n",
    "    model.summary()\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/05_inception/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
