{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 784)\n",
      "y train (60000, 10)\n",
      "x test (10000, 784)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalization of dataset\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# flatten the dataset\n",
    "x_train = x_train.reshape((-1, 28 * 28))\n",
    "x_test = x_test.reshape((-1, 28 * 28))\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression in Eager\n",
    "This is the standard linear classifier that is the easiest to build in almost all frameworks.\n",
    "\n",
    "In Keras, its even easier with a single `Dense` layer doing all of the important work.\n",
    "\n",
    "## The Catch\n",
    "\n",
    "Normally, we output the raw logits without an activation function, and then use `tf.nn.softmax_cross_entropy_with_logits` to use softmax and calculate the loss in one go.\n",
    "\n",
    "We however are going to use Model.fit(), which will **not** be using `tf.nn.softmax_cross_entropy_with_logits`, and therefore we need to use `softmax` activation function for the final layer.\n",
    "\n",
    "Now another hurdle. The softmax op doesnt exist for the GPU, only the CPU. This is easy enough to fix thankfully. Simply force a `with tf.device('/cpu:0'):` over the softmax activation to force it onto the CPU.\n",
    "\n",
    "Note, for Keras, it is important **not to use activation='softmax' for the final Dense layer**. Since the layer will be on the GPU it will try to use softmax activation of the GPU as well, and cause an exception. For Eager mode, stick to using the activation seperately in another block.\n",
    "\n",
    "## A word on performance\n",
    "Tensorflow 1.8 now automatically places operations in CPU or GPU silently, which is great for useability, bad for maximum performance. When using Tensorflow without eager, we generally let Tensorflow decide where to place ops, since the graph is later optimized to get near optimal performance anyway.\n",
    "\n",
    "Eager doesnt bother with graphs, so not much room for optimizations like that. Instead, I will be checking and forcing the entire training or testing loop onto the available device. \n",
    "\n",
    "If you don't want this, or would rather not bother with performance finetuning like this, then you can do so. However, when speed on the GPU is comparared to other frameworks, say PyTorch, this is absolutely important, especially for certain models like custom RNNs, I have noticed speed ups over over 4x when forcing everything - the model, the optimizer, fit, predict and evaluate calls onto the GPU, and it ranks quite closely with PyTorch at that level. \n",
    "\n",
    "This is ofcourse just an example, it wont hold true for everything, but its nice to use the GPU at maximum possible utilization if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition (canonical way)\n",
    "class LogisticRegression(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        output = self.dense(inputs)\n",
    "\n",
    "        # softmax op does not exist on the gpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.5850 - acc: 0.8504 - val_loss: 0.3811 - val_acc: 0.8988\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.3759 - acc: 0.8966 - val_loss: 0.3356 - val_acc: 0.9080\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.3436 - acc: 0.9041 - val_loss: 0.3181 - val_acc: 0.9129\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.3269 - acc: 0.9091 - val_loss: 0.3055 - val_acc: 0.9153\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.3160 - acc: 0.9114 - val_loss: 0.3022 - val_acc: 0.9179\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.3083 - acc: 0.9135 - val_loss: 0.2926 - val_acc: 0.9164\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.3025 - acc: 0.9155 - val_loss: 0.2903 - val_acc: 0.9191\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.2978 - acc: 0.9169 - val_loss: 0.2880 - val_acc: 0.9205\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.2940 - acc: 0.9184 - val_loss: 0.2854 - val_acc: 0.9211\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.2905 - acc: 0.9187 - val_loss: 0.2834 - val_acc: 0.9199\n",
      "Final test loss and accuracy : [0.2833801755428314, 0.9199]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = LogisticRegression(num_classes)\n",
    "    model.compile(optimizer=tf.train.GradientDescentOptimizer(0.1), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = np.zeros((1, 28 * 28))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=2)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=2)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
