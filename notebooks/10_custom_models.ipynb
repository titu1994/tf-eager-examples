{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (404, 13) 3.6316616e-10 1.0\n",
      "y train (404,) 22.395049504950492 9.199035423364862\n",
      "x test (102, 13) 0.02082699 0.98360837\n",
      "y test (102,) 23.07843137254902 9.123806690181466\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "batch_size = 128\n",
    "epochs = 26\n",
    "\n",
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# normalization of dataset\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "\n",
    "x_train = (x_train - mean) / (std + 1e-8)\n",
    "x_test = (x_test - mean) / (std + 1e-8)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x train', x_train.shape, x_train.mean(), x_train.std())\n",
    "print('y train', y_train.shape, y_train.mean(), y_train.std())\n",
    "print('x test', x_test.shape, x_test.mean(), x_test.std())\n",
    "print('y test', y_test.shape, y_test.mean(), y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Models\n",
    "Here, we are going to try to mix Keras Layers with Tensorflow Variables. \n",
    "\n",
    "Keras has a nice function - \"add_variable\" and \"add_weights\" which can be used in custom layers or models to register weight matrices generated from the backend independent of Keras. However, this feature has not yet been implemented in tf.keras for Eager mode. \n",
    "\n",
    "Therefore, we are going to do this completely in base Eager style code - defining our own gradients function, our own train and evaluate loop and model saving and loading.\n",
    "\n",
    "Hopefully, this can be fixed in later versions of Tensorflow, so that we can just use the \"self.add_variable(..)\" inside our Model classes and go back to using Model.fit() and Model.predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class CustomRegressor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomRegressor, self).__init__()\n",
    "        # self.add_variable and self.add_weight are not yet supported\n",
    "        self.custom_variables = OrderedDict()\n",
    "\n",
    "        # we also use a keras layer along with a custom weight matrix\n",
    "        self.hidden2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        if 'hidden' not in self.custom_variables:\n",
    "            # this is equivalent to a Dense layer from Keras (same as hidden2)\n",
    "            hidden = tf.get_variable('hidden1', shape=[inputs.shape[-1], 1], dtype=tf.float32,\n",
    "                                     initializer=tf.keras.initializers.he_normal())\n",
    "            self.custom_variables['hidden'] = hidden\n",
    "\n",
    "        output1 = tf.matmul(inputs, self.custom_variables['hidden'])\n",
    "        output1 = tf.nn.relu(output1)\n",
    "\n",
    "        output2 = self.hidden2(inputs)\n",
    "\n",
    "        output = output1 + output2  # goofy layer ; just for demonstration purposes\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients\n",
    "Normally, Keras handles the gradient computation for us. However, for our hybrid model, only the Keras Dense layer is registered. The weights stored in the `custom_variables` dictionary are not managed by Keras at all, so we cannot depend on Keras' Model.fit() to update those weights.\n",
    "\n",
    "Therefore, we go around this by using tf.GradientTape() to monitor and compute the gradients of all the variables inside the model, and then pass the loss and the grad_vars as the result.\n",
    "\n",
    "## Note\n",
    "It is important to remember that we need to explicitely pass a list of custom variables to the `tape.gradients()` function.\n",
    "\n",
    "Here, we use an OrderedDict to manage our custom variables, and therefore we can access the variables using OrderedDict.values(). We then append these custom variables to the variables that are managed by Keras (inside `model.variables`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(x)\n",
    "        loss = tf.losses.mean_squared_error(y, outputs[:, 0])\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.variables + list(model.custom_variables.values()))\n",
    "    grad_vars = zip(gradients, model.variables + list(model.custom_variables.values()))\n",
    "    return loss, grad_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop\n",
    "\n",
    "Since Keras does not directly manage all of the variables, we can't rely on the Model.fit() method anymore. So we create a training loop using the best practices of Eager execution.\n",
    "\n",
    "These best practices are : \n",
    "- Using tf.data.Dataset() apis to manage the train and test sets\n",
    "- Using Eager metrics (tfe.metrics) to calculate the per iteration loss\n",
    "- Use tfe.Saver() to save the weights of the model\n",
    "\n",
    "## Training Loop Phases\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "In the training phase, we loop over the entire training dataset, compute the gradients, use the Optimizer to apply those gradients and then update our metrics.\n",
    "\n",
    "For generating batches using Dataset API, it is beneficial to use an infinite generator and manage the cutoff points of the inner loop ourselves. Also, preprocessing can be done with .map() functions, which aren't needed here.\n",
    "\n",
    "For metrics, it is useful to use tfe.metrics.Accuracy() or tfe.metrics.Mean() to compute the categorical accuracy or mean over the entire training set.\n",
    "\n",
    "### Test Phase\n",
    "\n",
    "In the test phase, we loop over the test dataset exactly one time. This is ensured by using Dataset.make_one_shot_iterator(). In Eager mode, when you use this, you can loop over the provided iterator just like normal, and it will generate the batches for the entire set exactly once, so no need to bother with clipping off the loop.\n",
    "\n",
    "## Saving Weights\n",
    "When saving weights of a hybrid model, several important points must be remembered.\n",
    "\n",
    "- Keras maintains an automatic internal naming scheme. Therefore, when you create 2 models, they **will have different names for the Keras Layers**. Custom Variables are not affected by this thankfully, but this causes an issue when loading weights.\n",
    "    - Since checkpoints depend on the layer names for saving and restoring, creating saving a model (with say subscript `_1` for all layers) and then attempting to restore a second instance of this model (with subscript `_2` for all layers) will throw an Error. The checkpoint cannot find the weights with the layer names (since it stores the layers with name subscript `_1`)\n",
    "    - Easy fix = Use **tf.keras.backend.clear_session()** before restoring and creating the second model. This will reset the name counter to `_1`.\n",
    "\n",
    "\n",
    "- Model weights and custom Variables are built only after the 1st call. \n",
    "    - When training, variables building is done automatically when we pass the batch of training samples so we dont have to worry when saving the model.\n",
    "    - However, when we create a second model to restore, the weights are not yet built. We need to pass a dummy batch to force the model to finish building all of its weights with correct shapes.\n",
    "    - This is also true for Keras models with **only Keras layers/submodels** when in Eager mode. A Keras model with only Keras layers is also not completely built unless you use Model._set_input(dummy_batch) on it or call it with a dummy batch before restoring weights to it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 304.1588 | Test Loss = 215.8727\n",
      "\n",
      "Epoch 2: Train Loss = 224.8164 | Test Loss = 98.4446\n",
      "\n",
      "Epoch 3: Train Loss = 78.5605 | Test Loss = 76.8298\n",
      "\n",
      "Epoch 4: Train Loss = 75.1571 | Test Loss = 82.2543\n",
      "\n",
      "Epoch 5: Train Loss = 129.3626 | Test Loss = 76.2721\n",
      "\n",
      "Epoch 6: Train Loss = 105.4899 | Test Loss = 56.4735\n",
      "\n",
      "Epoch 7: Train Loss = 42.5363 | Test Loss = 54.0988\n",
      "\n",
      "Epoch 8: Train Loss = 44.8493 | Test Loss = 30.6633\n",
      "\n",
      "Epoch 9: Train Loss = 63.8818 | Test Loss = 26.2789\n",
      "\n",
      "Epoch 10: Train Loss = 28.5036 | Test Loss = 33.6216\n",
      "\n",
      "Epoch 11: Train Loss = 25.2791 | Test Loss = 16.5017\n",
      "\n",
      "Epoch 12: Train Loss = 39.0379 | Test Loss = 18.6802\n",
      "\n",
      "Epoch 13: Train Loss = 31.4691 | Test Loss = 19.9136\n",
      "\n",
      "Epoch 14: Train Loss = 20.5009 | Test Loss = 17.1845\n",
      "\n",
      "Epoch 15: Train Loss = 24.8141 | Test Loss = 15.9176\n",
      "\n",
      "Epoch 16: Train Loss = 28.9241 | Test Loss = 15.6662\n",
      "\n",
      "Epoch 17: Train Loss = 22.7698 | Test Loss = 15.2226\n",
      "\n",
      "Epoch 18: Train Loss = 20.3520 | Test Loss = 14.0791\n",
      "\n",
      "Epoch 19: Train Loss = 23.5152 | Test Loss = 14.1138\n",
      "\n",
      "Epoch 20: Train Loss = 21.8741 | Test Loss = 15.0250\n",
      "\n",
      "Epoch 21: Train Loss = 16.4936 | Test Loss = 15.3754\n",
      "\n",
      "Epoch 22: Train Loss = 16.0606 | Test Loss = 13.3169\n",
      "\n",
      "Epoch 23: Train Loss = 16.7683 | Test Loss = 13.1751\n",
      "\n",
      "Epoch 24: Train Loss = 16.2061 | Test Loss = 14.0430\n",
      "\n",
      "Epoch 25: Train Loss = 15.2370 | Test Loss = 14.5987\n",
      "\n",
      "Epoch 26: Train Loss = 15.1741 | Test Loss = 14.1264\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = CustomRegressor()\n",
    "    \n",
    "    dummy_x = tf.zeros([1] + [x_train.shape[-1]])\n",
    "    model.call(dummy_x)\n",
    "\n",
    "    # Can no longer use Keras utility functions since we could not register the variable to keras properly\n",
    "    # Whenever TF allows the addition of variables using Keras APIs, this will become easier like before\n",
    "    optimizer = tf.train.AdamOptimizer(1.0)\n",
    "\n",
    "    # wrap with datasets to make life slightly easier\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.batch(batch_size).shuffle(100).repeat().prefetch(20)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "    # train the model\n",
    "    num_batch_per_epoch = len(x_train) // batch_size + 1\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # measure the losses\n",
    "        train_loss = tfe.metrics.Mean()\n",
    "        test_loss = tfe.metrics.Mean()\n",
    "\n",
    "        for b, (x, y) in enumerate(train_dataset):\n",
    "            loss, grads = gradients(model, x, y)\n",
    "            optimizer.apply_gradients(grads, tf.train.get_or_create_global_step())\n",
    "\n",
    "            # update the running training loss\n",
    "            train_loss(loss)\n",
    "\n",
    "            if b >= num_batch_per_epoch:\n",
    "                break\n",
    "\n",
    "        # evaluate after epoch\n",
    "        iterator = test_dataset.make_one_shot_iterator()  # dont repeat any values from test set\n",
    "        for x, y in iterator:\n",
    "            preds = model(x)\n",
    "            loss = tf.losses.mean_squared_error(y, preds[:, 0])\n",
    "\n",
    "            test_loss(loss)\n",
    "\n",
    "        print(\"Epoch %d: Train Loss = %0.4f | Test Loss = %0.4f\\n\" % (e + 1, train_loss.result(), test_loss.result()))\n",
    "\n",
    "    # Make sure to add not just the \"model\" variables, but also the custom variables we added !\n",
    "    saver = tfe.Saver(model.variables + list(model.custom_variables.values()))\n",
    "    saver.save('weights/10_custom_models/weights.ckpt')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore the weights to the model\n",
    "\n",
    "- As suggested above, we clear the backend to reset the naming counter in tf.keras, then we generate the model.\n",
    "- We build the model using a dummy batch\n",
    "- Load a new Saver object, and pass the custom variables and Layer variables to this Saver and restore\n",
    "- After this, we can use the model just as during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from weights/10_custom_models/weights.ckpt\n",
      "Weights restored\n",
      "Test Loss = 14.1264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear the previous session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "with tf.device(device):    \n",
    "    # Now we restore the model and predict again on test set\n",
    "    model2 = CustomRegressor()\n",
    "\n",
    "    # we need to run the model at least once to build all of the variables and the custom variables\n",
    "    # make sure to build the model the same way, otherwise it wont find the weights in the checkpoints properlyoperly\n",
    "    # safest option is to call model.call(tf_input_batch) explicitly\n",
    "    dummy_x = tf.zeros([1] + [x_train.shape[-1]])\n",
    "    model2.call(dummy_x)\n",
    "    \n",
    "    # ensure that you are loading both the Keras variables AND the custom variables\n",
    "    saver2 = tfe.Saver(model2.variables + list(model2.custom_variables.values()))\n",
    "    saver2.restore('weights/10_custom_models/weights.ckpt')\n",
    "    print(\"Weights restored\")\n",
    "\n",
    "    # evaluate the results\n",
    "    iterator = test_dataset.make_one_shot_iterator()  # dont repeat any values from test set\n",
    "    test_loss = tfe.metrics.Mean()\n",
    "\n",
    "    for x, y in iterator:\n",
    "        preds = model2(x)\n",
    "        loss = tf.losses.mean_squared_error(y, preds[:, 0])\n",
    "\n",
    "        test_loss(loss)\n",
    "\n",
    "    print(\"Test Loss = %0.4f\\n\" % (test_loss.result()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
