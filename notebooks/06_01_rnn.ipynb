{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "units = 128\n",
    "batch_size = 100\n",
    "epochs = 2\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28)\n",
      "y train (60000, 10)\n",
      "x test (10000, 28, 28)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timestep\n",
    "x_test = x_test.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timeste\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical RNN \n",
    "\n",
    "This is the suggested way to use Keras RNNs - create the Cells, wrap with an RNN and then simply call the inputs on the RNN. \n",
    "\n",
    "However, in practice, this is several times slower than expected. The simplest explanation is the call to `K.rnn()` inside the RNN class, which is used to symbolically loop over timesteps when Tensorflow is not running in Eager mode.\n",
    "\n",
    "This symbolic operation severely hampers the performance of the model. One approach to somewhat reduce this issue is to write the loop ourselves. There is an example in `06_03_rnn.ipynb`, where I take a LSTMCell and write the pythonic loop and call the cell on the slices of data manually. It is faster than the 1 layer version that is shown here, but still alot slower than the `BasicLSTM` that I wrote - which is just the LSTM layer in Eager code from scratch. \n",
    "\n",
    "The performance of BasicLSTM is almost the same, has none of the frills of the Keras LSTM but its 1 layer and 2 layer versions are significantly faster than the version presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, units, num_classes, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.cells = [tf.keras.layers.LSTMCell(units, implementation=2) for _ in range(num_layers)]  # Use GPU implementation\n",
    "        self.rnn = tf.keras.layers.RNN(self.cells, unroll=True)  # extremely slow if not unrolled - probably because it is using K.rnn() internally.\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # this approach is canonical, but slow\n",
    "        # probably will be fixed in later versions of Tensorflow\n",
    "        \n",
    "        x = self.rnn(inputs)\n",
    "        output = self.classifier(x)\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 349s 6ms/step - loss: 0.3126 - acc: 0.9010 - val_loss: 0.1028 - val_acc: 0.9713\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 349s 6ms/step - loss: 0.0937 - acc: 0.9731 - val_loss: 0.0884 - val_acc: 0.9737\n",
      "10000/10000 [==============================] - 16s 2ms/step\n",
      "Final test loss and accuracy : [0.08836719910963438, 0.9737000066041946]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = RNN(units, num_classes, num_layers=2)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = tf.zeros((1, 28, 28))\n",
    "    model.call(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/06_01_rnn/weights.ckpt')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
