{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "units = 128\n",
    "batch_size = 100\n",
    "epochs = 2\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28)\n",
      "y train (60000, 10)\n",
      "x test (10000, 28, 28)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timestep\n",
    "x_test = x_test.reshape((-1, 28, 28))  # 28 timesteps, 28 inputs / timeste\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast LSTM Cell\n",
    "\n",
    "Here, we take a middle ground approach to the canonical (6.1) slow approach and the custom (6.2) approach to building RNNs. \n",
    "\n",
    "It is not recommended to try building RNNs from scratch unless you know what is going on in the model and many factors like initialization, order of operations, activation function can affect results drastically (it is not as fun as it looks). \n",
    "\n",
    "Hence, when you want something pre-built, but with the speed comparable to a custom model, you can use the Cell variant of that RNN layer. \n",
    "\n",
    "Here, we will use the LSTMCell from Keras to demonstrate this. Note, that this is a full fledged LSTM Cell from Keras, therefore it has a lot more niceties like dropout, recurrent dropout and initializers, constraints and regularizers etc. \n",
    "\n",
    "## Notes\n",
    "\n",
    "A point to take not of is that since a cell was originally meant to be built by the RNN that wraps it, we have to perform an extra check inside `call` to build the RNN if it has not been built the first time. This will make the first call to the Model slightly slower, but not by much.\n",
    "\n",
    "In essence, we override the K.rnn() symbolic loop and use Pythonic loop to manage the internal state of the lstm as well as the feeding of data slices to it.\n",
    "\n",
    "It won't take much to extend this to Multi layer RNN. \n",
    "\n",
    "- Here we have 2 initial states for 1 layer. For k layers, you would need a list of 2 * k initial states. This is specific to an LSTM. A GRU needs only 1 state.\n",
    "- Loop around the 1st Cell and manage its output embedding. Here, we wont maintain just the final `x`, but the entire list of `x` over all timesteps, and use `tf.concat(x_list, axis=1)` to get back a time series of shape (batchsize, timesteps, hiddendim)\n",
    "- After this first layer, use the outer loop to switch to the next cell. Initially, the states of this cell will be the two zero states corresponding to the i*2-th location in the state list.\n",
    "- Loop around while maintaining all of its intermediate `x` and do the same as above.\n",
    "- Finally, after all the layers of the RNN are done, feed only the final x of the final Cell to the classifier.\n",
    "\n",
    "## Noticeable speed difference\n",
    "\n",
    "Even with the loop using Python alone, this model is slightly slower than the barebones BasicLSTM model. I have no idea why this is the case. Fundamentally, what I wrote for `BasicLSTM` is actually just a LSTM cell, which we use directly here. When I use the Keras `LSTMCell`, it should be equivalent, without the excess stuff like dropout etc. That alone should not account for such a speedup.\n",
    "\n",
    "**NOTE:** \n",
    "If the GPU is available, you should use `implementation=2` for maximum speed on the GPU. This, however, reduces regularization somewhat so be careful about finding good values for `dropout` and `recurrent_dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, units, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.lstm_cell = tf.nn.rnn_cell.LSTMCell(units)  # use the tensorflow cell directly\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        state = self.lstm_cell.zero_state(batch_size=inputs.shape[0], dtype=tf.float32)\n",
    "        x = inputs\n",
    "\n",
    "        # if the LSTMCell is being called for the first time ever\n",
    "        # Build the cell's weights manually\n",
    "        if not self.lstm_cell.built:\n",
    "            self.lstm_cell._dtype = tf.float32  # important : force set the data type via its private variable\n",
    "            self.lstm_cell.build(inputs.shape[1:])\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            input = inputs[:, t, :]  # extract the current input at timestep t\n",
    "            x, state = self.lstm_cell(input, state=state)  # get the output embedding and the states ; note `state` rather than `states`\n",
    "\n",
    "            # states = feed in the states back to the next timestep\n",
    "\n",
    "        output = self.classifier(x)  # feed the last `x` as the hidden embedding of the lstm to the classifier\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.3065 - acc: 0.9026 - val_loss: 0.1253 - val_acc: 0.9624\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.0921 - acc: 0.9729 - val_loss: 0.0722 - val_acc: 0.9790\n",
      "10000/10000 [==============================] - 4s 436us/step\n",
      "Final test loss and accuracy : [0.07217171450378373, 0.9790000069141388]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = RNN(units, num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = tf.zeros((1, 28, 28))\n",
    "    model.call(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/06_03_rnn/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
