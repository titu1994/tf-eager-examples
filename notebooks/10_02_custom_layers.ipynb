{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "batch_size = 128\n",
    "epochs = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (404, 13) 3.6316616e-10 1.0\n",
      "y train (404,) 22.395049504950492 9.199035423364862\n",
      "x test (102, 13) 0.02082699 0.98360837\n",
      "y test (102,) 23.07843137254902 9.123806690181466\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# normalization of dataset\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "\n",
    "x_train = (x_train - mean) / (std + 1e-8)\n",
    "x_test = (x_test - mean) / (std + 1e-8)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x train', x_train.shape, x_train.mean(), x_train.std())\n",
    "print('y train', y_train.shape, y_train.mean(), y_train.std())\n",
    "print('x test', x_test.shape, x_test.mean(), x_test.std())\n",
    "print('y test', y_test.shape, y_test.mean(), y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers\n",
    "\n",
    "There is currently no way to add custom weight matrices or custom variables to a `tf.keras.Model`. However, we *can* quitely easily work around that by simply extending a **tf.keras.layers.Layer** !\n",
    "\n",
    "A Custom Layer is in essence a Layer that we can treat as if it was a part of the Keras API itself. This includes the ability to build the model on demand, and register any layer automatically without having to resort to the messy hack that we had to do in `10_01_custom_models.ipynb`. \n",
    "\n",
    "There are 3 important functions (and 1 optional function) which must be overridden to write a custom layer : \n",
    "\n",
    "- `__init__` : Must pass **kwargs to its super constructior to maintain Keras API conventions\n",
    "\n",
    "\n",
    "- `build(self, _)` : This must be extended, and the `_` must be replaced by the input_shape like I have done here. Use this input shape to build the variables of the model. \n",
    "    - Note : **It is important to set `self.built = True` at the end of build(). Otherwise, multiple copies of the weights with the same name will be built at each call of the layer, which will cause wrong training and the model wont be Checkpointable.**.\n",
    "    - `self.built = True` can be automatically be set by calling the base method `super().build()`. It doesnt need to be passed the input shape, but if it is passed, it will be ignored and simply set `self.built = True` for us.\n",
    "\n",
    "\n",
    "- `call(self, inputs, **kwargs)` : The same as models, but here, trainable and mask parameters come under **kwargs. They can be set as dictionary values in the call method. \n",
    "    - Use the variables and weights you build in side `build()` to perform the forward pass here.\n",
    "\n",
    "\n",
    "- `compute_output_shape(self, input_shape)` : Optional override. Allows you to define the output shape of the layer. While this is not needed for Eager execution mode, it is **mandatory** for normal execution mode and ordinary Keras layers. Takes in the input_shape as a tuple and passes a tuple of integer shapes as its outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"Custom\" layer which mimics the Dense layer from Keras\n",
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "\n",
    "    # change the \"_\" for the input shape to some variable name, and build on first call !\n",
    "    def build(self, input_shape):\n",
    "        # add variable / add_weights works inside Layers but not Models !\n",
    "        self.kernel = self.add_variable('kernel',\n",
    "                                        shape=[input_shape[-1], self.dim],\n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.keras.initializers.he_normal())\n",
    "\n",
    "        # Do NOT forget to call this line, otherwise multiple model variables will be built with the same name\n",
    "        # This cannot happen inside Keras layers, and therefore the model will not be Checkpointeable.\n",
    "        # It also wont train properly.\n",
    "        #\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.matmul(inputs, self.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Custom Layer\n",
    "\n",
    "Now that the layer has been written, use it just like any other custom layer written in Keras. It is now an extention to the Keras API, so all the same rules apply to the custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class CustomRegressor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomRegressor, self).__init__()\n",
    "        # self.add_variable and self.add_weight are not yet supported inside a Model\n",
    "        # However, since we created a custom layer (Dense layer), we *can* attach it to this model\n",
    "        # just like other layers !\n",
    "        self.hidden1 = CustomLayer(1)\n",
    "\n",
    "        # we also use a keras layer along with a custom weight matrix\n",
    "        self.hidden2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        output1 = self.hidden1(inputs)\n",
    "        output1 = tf.keras.activations.relu(output1)\n",
    "\n",
    "        output2 = self.hidden2(inputs)\n",
    "\n",
    "        output = output1 + output2  # goofy model ; just for demonstration purposes\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefit\n",
    "\n",
    "With the inbuilt support for all Keras layers, a Keras model can now use *all of the additional functionality* such as Model.fit() and Model.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404 samples, validate on 102 samples\n",
      "Epoch 1/24\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 439.0670 - val_loss: 129.8505\n",
      "Epoch 2/24\n",
      "404/404 [==============================] - 0s 93us/step - loss: 96.3968 - val_loss: 92.0870\n",
      "Epoch 3/24\n",
      "404/404 [==============================] - 0s 95us/step - loss: 87.9206 - val_loss: 102.1922\n",
      "Epoch 4/24\n",
      "404/404 [==============================] - 0s 99us/step - loss: 85.3385 - val_loss: 63.0205\n",
      "Epoch 5/24\n",
      "404/404 [==============================] - 0s 101us/step - loss: 48.4697 - val_loss: 39.8241\n",
      "Epoch 6/24\n",
      "404/404 [==============================] - 0s 109us/step - loss: 38.2497 - val_loss: 44.6685\n",
      "Epoch 7/24\n",
      "404/404 [==============================] - 0s 98us/step - loss: 42.0978 - val_loss: 36.3873\n",
      "Epoch 8/24\n",
      "404/404 [==============================] - 0s 101us/step - loss: 34.2104 - val_loss: 30.5881\n",
      "Epoch 9/24\n",
      "404/404 [==============================] - 0s 105us/step - loss: 26.8764 - val_loss: 30.7118\n",
      "Epoch 10/24\n",
      "404/404 [==============================] - 0s 104us/step - loss: 28.4332 - val_loss: 30.6307\n",
      "Epoch 11/24\n",
      "404/404 [==============================] - 0s 108us/step - loss: 25.1700 - val_loss: 27.3655\n",
      "Epoch 12/24\n",
      "404/404 [==============================] - 0s 124us/step - loss: 22.7601 - val_loss: 23.1679\n",
      "Epoch 13/24\n",
      "404/404 [==============================] - 0s 103us/step - loss: 24.3568 - val_loss: 25.7230\n",
      "Epoch 14/24\n",
      "404/404 [==============================] - 0s 109us/step - loss: 22.3994 - val_loss: 22.7353\n",
      "Epoch 15/24\n",
      "404/404 [==============================] - 0s 110us/step - loss: 20.1189 - val_loss: 19.8217\n",
      "Epoch 16/24\n",
      "404/404 [==============================] - 0s 103us/step - loss: 19.5704 - val_loss: 21.8405\n",
      "Epoch 17/24\n",
      "404/404 [==============================] - 0s 110us/step - loss: 20.8592 - val_loss: 18.3901\n",
      "Epoch 18/24\n",
      "404/404 [==============================] - 0s 110us/step - loss: 18.7998 - val_loss: 19.0950\n",
      "Epoch 19/24\n",
      "404/404 [==============================] - 0s 94us/step - loss: 17.6288 - val_loss: 16.9345\n",
      "Epoch 20/24\n",
      "404/404 [==============================] - 0s 99us/step - loss: 17.1514 - val_loss: 18.7752\n",
      "Epoch 21/24\n",
      "404/404 [==============================] - 0s 98us/step - loss: 16.7026 - val_loss: 17.5502\n",
      "Epoch 22/24\n",
      "404/404 [==============================] - 0s 96us/step - loss: 16.9528 - val_loss: 16.1911\n",
      "Epoch 23/24\n",
      "404/404 [==============================] - 0s 108us/step - loss: 16.3410 - val_loss: 17.1234\n",
      "Epoch 24/24\n",
      "404/404 [==============================] - 0s 105us/step - loss: 17.2114 - val_loss: 15.9176\n",
      "Test MSE : 15.917597770690918\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = CustomRegressor()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(1.), loss='mse')\n",
    "\n",
    "    # suggested fix for TF <= 1.9; can be incorporated inside `_eager_set_inputs` or `_set_input`\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = tf.zeros((1, 13))\n",
    "    model.call(dummy_x)\n",
    "\n",
    "    # Now that we have a \"proper\" Keras layer, we can rely on Model utility functions again !\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "    print(\"Test MSE :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/10_02_custom_layers/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
