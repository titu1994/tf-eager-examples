{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.contrib.eager.python import tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28, 1)\n",
      "y train (60000, 10)\n",
      "x test (10000, 28, 28, 1)\n",
      "y test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n",
    "# and tensors as input to keras\n",
    "y_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "A basic conv net with 2 `Conv-BatchNorm-Relu` blocks. This is the canonical way to define a small network.\n",
    "\n",
    "However, we can also decompose the blocks themselves into another class which also extends Model, and then use objects of that Model inside another Model. This is allowed, and follows OOP principles, so I will be using this pattern from now on.\n",
    "\n",
    "The example of Model as a layer can be seen in `04_02_cnn.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition (canonical way)\n",
    "class CNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = tf.keras.layers.Conv2D(16, (5, 5), padding='same', strides=(2, 2),\n",
    "                                           kernel_initializer='he_normal')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.cnn2 = tf.keras.layers.Conv2D(32, (5, 5), padding='same', strides=(2, 2),\n",
    "                                           kernel_initializer='he_normal')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.cnn1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)  # layer 1\n",
    "        x = tf.nn.relu(self.bn2(self.cnn2(x)))  # layer 2\n",
    "        x = self.pool(x)\n",
    "        output = self.classifier(x)\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 1.3410 - acc: 0.6647 - val_loss: 0.9821 - val_acc: 0.7298\n",
      "Epoch 2/8\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.5720 - acc: 0.8861 - val_loss: 0.5948 - val_acc: 0.8021\n",
      "Epoch 3/8\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.3462 - acc: 0.9251 - val_loss: 0.4176 - val_acc: 0.8774\n",
      "Epoch 4/8\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.2550 - acc: 0.9419 - val_loss: 0.3381 - val_acc: 0.9110\n",
      "Epoch 5/8\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.2048 - acc: 0.9513 - val_loss: 0.2127 - val_acc: 0.9420\n",
      "Epoch 6/8\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.1732 - acc: 0.9580 - val_loss: 0.2316 - val_acc: 0.9397\n",
      "Epoch 7/8\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.1517 - acc: 0.9626 - val_loss: 0.1862 - val_acc: 0.9456\n",
      "Epoch 8/8\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.1336 - acc: 0.9661 - val_loss: 0.2105 - val_acc: 0.9353\n",
      "10000/10000 [==============================] - 1s 51us/step\n",
      "Final test loss and accuracy : [0.21054935339689254, 0.9353]\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    # build model and optimizer\n",
    "    model = CNN(num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TF Keras tries to use entire dataset to determine shape without this step when using .fit()\n",
    "    # Fix = Use exactly one sample from the provided input dataset to determine input/output shape/s for the model\n",
    "    dummy_x = np.zeros((1, 28, 28, 1))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
